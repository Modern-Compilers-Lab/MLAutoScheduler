{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8da4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch_mlir\n",
    "from torch_mlir_e2e_test.linalg_on_tensors_backends.refbackend import RefBackendLinalgOnTensorsBackend\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22a6948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_load_on_refbackend(module):\n",
    "    backend = RefBackendLinalgOnTensorsBackend()\n",
    "    compiled = backend.compile(module)\n",
    "    return backend.load(compiled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39033be",
   "metadata": {},
   "source": [
    "# Example on the Resnet18 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905df13",
   "metadata": {},
   "source": [
    "### 1. Manual Conversion (to get the IRs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f61afd",
   "metadata": {},
   "source": [
    "Load the model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5933c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e257b1",
   "metadata": {},
   "source": [
    "Create a random example of input, for \"torch-mlir\" to infer the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb6a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.ones(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5689d41",
   "metadata": {},
   "source": [
    "Compile the model using the random example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dee98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled = torch_mlir.compile(model, example_input, output_type=\"linalg-on-tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aaba17",
   "metadata": {},
   "source": [
    "Save the compiled model in an MLIR file. \n",
    "Note: the resulting IR is not executable yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "811afb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"mlir_files/resnet18.mlir\"\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write(str(compiled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03d54e",
   "metadata": {},
   "source": [
    "Execute the conversion script (that fixes, wraps, bufferizes and lowers the model into an executable format). The script returns 0 when done.\n",
    "\n",
    "You can inspect the resulting files of every step in the \"mlir_files\" folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caf74510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./mlir_files/resnet18.mlir:641:5: error: redefinition of symbol named 'nanoTime'\n",
      "    func.func private @nanoTime() -> i64 attributes {llvm.emit_c_interface}\n",
      "    ^\n",
      "./mlir_files/resnet18.mlir:641:5: note: see current operation: \n",
      "\"func.func\"() <{function_type = () -> i64, sym_name = \"nanoTime\", sym_visibility = \"private\"}> ({\n",
      "}) {llvm.emit_c_interface} : () -> ()\n",
      "./mlir_files/resnet18.mlir:612:5: note: see existing symbol definition here\n",
      "    func.func private @nanoTime() -> i64 attributes {llvm.emit_c_interface}\n",
      "    ^\n",
      "./mlir_files/resnet18.mlir:641:5: error: redefinition of symbol named 'nanoTime'\n",
      "    func.func private @nanoTime() -> i64 attributes {llvm.emit_c_interface}\n",
      "    ^\n",
      "./mlir_files/resnet18.mlir:641:5: note: see current operation: \n",
      "\"func.func\"() <{function_type = () -> i64, sym_name = \"nanoTime\", sym_visibility = \"private\"}> ({\n",
      "}) {llvm.emit_c_interface} : () -> ()\n",
      "./mlir_files/resnet18.mlir:612:5: note: see existing symbol definition here\n",
      "    func.func private @nanoTime() -> i64 attributes {llvm.emit_c_interface}\n",
      "    ^\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(command=\"./convert.sh resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e9f66",
   "metadata": {},
   "source": [
    "You can now execute the model using the MLIR backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "617f5663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unranked Memref base@ = 0x573d798c7200 rank = 2 offset = 0 sizes = [1, 1000] strides = [1000, 1] data = \n",
      "[[0.309578,   -0.361994,   -1.2193,   -0.964296,   0.111097,   0.710505,   -1.25162,   -0.810416,   -1.48079,   -0.484123,   -0.0440142,   -1.37726,   -1.3669,   -0.954988,   -1.2015,   -0.762648,   -0.10179,   -1.35949,   -1.08807,   -1.18677,   -0.833686,   1.17233,   0.224612,   -0.346416,   -0.939225,   0.0787166,   -0.234258,   0.0810409,   0.478479,   -1.23911,   -0.889535,   -0.717601,   -0.484779,   -1.43571,   -0.325226,   -0.916341,   -0.397071,   -0.798738,   0.635524,   -0.949957,   -0.767533,   -0.436079,   -0.401912,   0.256277,   -0.235594,   0.558754,   -0.507873,   -0.157723,   -1.08709,   -1.24056,   -0.717526,   0.931817,   0.440858,   -0.359523,   -0.356868,   -0.955342,   -0.898017,   -1.14174,   -0.754364,   0.52962,   -0.0386584,   -0.329822,   0.552272,   0.532207,   0.336979,   -0.135797,   0.347811,   -0.682906,   0.848631,   0.340697,   -1.11406,   1.26157,   -0.764008,   0.996624,   -1.49366,   0.509433,   -0.615013,   0.148919,   1.68593,   0.260093,   -0.183606,   -0.941811,   -0.291409,   -0.71453,   -1.40811,   -0.747629,   -0.591236,   -0.772669,   0.024333,   -0.255319,   -1.17411,   -0.123974,   0.833059,   -0.730934,   -1.18829,   -0.373668,   -0.514873,   -1.54051,   0.129592,   -0.389014,   -1.17824,   -0.745642,   0.175461,   1.41515,   -1.00862,   -2.31463,   -0.634253,   -0.940123,   -1.20034,   -1.30458,   -1.19134,   2.55835,   -0.112899,   0.392122,   -0.199249,   -1.12907,   -0.752704,   0.996109,   -0.911253,   -1.65875,   -0.42184,   -1.31977,   -0.575814,   -1.11384,   -0.349348,   -1.00119,   -0.117085,   -0.127449,   0.567847,   -0.436359,   -1.43043,   -0.994689,   -1.16986,   -0.674629,   -0.796652,   0.0930803,   -1.19628,   -1.00083,   -0.296468,   -1.37675,   -1.25124,   -0.578916,   -1.05929,   -0.785289,   -1.1987,   -1.28552,   0.0604986,   0.126054,   -1.49804,   -0.123284,   -1.43211,   0.196281,   0.0640432,   -0.283188,   -0.303697,   -0.822266,   -0.853563,   -0.270526,   -0.19439,   -0.416033,   -0.528545,   -0.292197,   -0.407294,   0.430831,   0.0431063,   0.0845533,   -0.406463,   -0.260914,   -0.34771,   -0.692034,   -0.808533,   -0.909457,   -0.324199,   -0.402585,   -0.917746,   -0.0754194,   -1.10949,   -0.65544,   -0.0827869,   -0.134264,   -0.935232,   -0.494242,   -0.764758,   -0.436737,   -0.213798,   0.135941,   -1.11891,   0.204434,   -0.0517149,   -0.461174,   0.455955,   -0.137327,   -0.963081,   -0.905635,   -0.47532,   0.175248,   -0.820977,   -0.636007,   -1.18032,   0.35026,   -0.878053,   -1.36657,   0.314975,   -0.479338,   -0.66994,   -0.813106,   0.260114,   0.129005,   0.00231151,   -0.0592714,   -0.0366827,   -0.446113,   -1.14238,   -0.951172,   -0.176017,   -0.353872,   0.20952,   -0.78502,   -1.1451,   -0.680833,   0.419743,   -0.328544,   0.0181281,   -0.246228,   -0.0783525,   -0.684714,   -0.728493,   -0.83099,   -0.497591,   -0.686833,   -1.00375,   -0.465572,   -0.185028,   -0.515629,   -0.136453,   0.156862,   -0.257987,   -0.687523,   -0.500796,   0.0178307,   0.245509,   -0.279045,   -0.680941,   -0.268175,   -1.21287,   -0.317247,   -0.616156,   0.121492,   -0.33658,   -0.357685,   -0.276764,   -1.21584,   0.178189,   -0.795296,   -0.0492918,   -1.18513,   -0.104153,   -0.319923,   0.0155445,   -0.18665,   -0.149831,   -0.163263,   0.229211,   -0.390137,   0.113585,   -0.737031,   -1.09421,   -0.604419,   -0.88445,   -0.257856,   -0.224225,   -0.657341,   -0.0276552,   -0.00757939,   0.516508,   0.479759,   0.0846591,   0.749116,   -0.094224,   0.533071,   0.232072,   0.896688,   -0.26896,   1.32601,   0.0380133,   0.447961,   -0.808738,   -0.403433,   0.0505515,   -1.13396,   -0.606713,   -0.592012,   -0.159747,   0.580737,   -0.66139,   -0.859913,   -0.83043,   -0.380476,   -0.04067,   0.543042,   -0.71014,   0.165411,   0.412171,   -0.0246077,   -0.973765,   0.114954,   0.705175,   -0.18878,   -0.190601,   -1.87235,   0.821425,   -0.194014,   -0.0744068,   -0.711191,   0.525564,   -0.182601,   -0.414923,   -1.00123,   0.048058,   0.0336008,   -0.523877,   -1.4401,   -1.71537,   -1.01239,   -1.11041,   -1.39665,   -0.336371,   -0.164991,   -0.682709,   -1.05102,   -0.22545,   0.346265,   -0.588439,   -1.24414,   -1.30169,   -1.07089,   -1.79377,   -0.347231,   -0.597309,   -1.0699,   -1.36997,   -0.730115,   -0.939613,   -0.848025,   -1.60281,   -1.13212,   -0.494634,   -1.08323,   -1.38431,   -0.815123,   -0.13736,   0.0092005,   -0.703114,   -0.408096,   -0.707064,   -1.6546,   1.00629,   0.417989,   0.650844,   0.635324,   -0.088877,   0.667773,   0.191675,   -0.168731,   0.0881285,   -0.810683,   -1.36889,   -0.363946,   -0.931964,   -0.851008,   -0.772507,   -0.455627,   -0.735999,   -0.222009,   -1.04407,   -1.06797,   -0.225692,   -0.681089,   -0.0269166,   -1.38635,   -1.12964,   -0.566666,   -0.532379,   -0.343384,   -1.45323,   -0.883576,   -1.02474,   -0.416413,   -0.90867,   0.852975,   -0.60275,   -0.278286,   -0.082116,   -2.21368,   -0.576452,   0.464454,   -1.7448,   -1.1303,   -0.120197,   1.89795,   0.844596,   -0.0797649,   0.931148,   -1.08018,   -0.628642,   0.358091,   -1.29185,   -1.33354,   -1.29521,   2.29244,   -1.69597,   -0.386332,   0.368288,   2.07018,   0.256527,   -1.82613,   2.19124,   -0.770449,   2.38865,   2.6178,   0.569289,   -0.543309,   1.0329,   1.8266,   -0.73849,   -0.235938,   0.607507,   1.17272,   -0.179576,   0.78591,   -0.548673,   0.256127,   1.17881,   -0.0804636,   -0.333395,   0.583251,   -1.0431,   -0.247365,   0.811961,   0.21466,   0.169521,   -0.432279,   -0.505222,   -0.0267141,   -0.565566,   -0.0388612,   1.83786,   0.849985,   -0.379732,   -0.301373,   -0.502634,   1.08011,   -0.850841,   0.553739,   -1.26694,   -0.590925,   1.54484,   0.654891,   -1.08211,   0.785271,   -0.898023,   0.510549,   1.75044,   0.865945,   1.95732,   1.39095,   -1.68779,   -1.91884,   -1.06075,   0.730627,   1.172,   -0.478113,   -0.17285,   3.38521,   -0.76164,   -0.959403,   -2.01626,   0.855783,   0.561936,   -0.206299,   -0.686748,   1.94401,   0.875201,   -0.930428,   -0.700004,   0.488758,   0.650079,   0.838523,   1.77147,   -1.15791,   0.199955,   2.1855,   0.364416,   0.453802,   1.50814,   -0.381892,   0.0127993,   -0.869274,   -0.683792,   3.27096,   -0.204649,   -0.204616,   1.09087,   1.19425,   0.228948,   0.251008,   -0.286658,   2.42571,   0.311405,   -1.86911,   -0.255726,   -1.15575,   3.61281,   0.639095,   1.29539,   0.32088,   0.709809,   0.538394,   0.351663,   0.234619,   -0.740514,   0.735284,   1.02348,   1.04406,   -0.10012,   -1.00191,   0.770378,   1.72692,   0.354299,   0.756545,   2.21657,   0.457578,   -0.175029,   -0.737921,   1.01341,   0.0399216,   -1.75349,   -0.611528,   -0.368957,   1.29415,   -0.271577,   0.790316,   0.895495,   1.40613,   -0.677039,   1.21503,   0.76908,   -1.55182,   -0.184094,   2.53744,   1.07533,   2.40223,   0.332256,   1.17494,   -1.40143,   -1.4993,   0.873713,   -0.293479,   0.479154,   1.76813,   -1.18281,   -0.011282,   -1.28555,   1.28235,   -1.18564,   -1.38307,   0.214722,   1.60182,   -0.348879,   -1.55366,   0.817121,   -1.22467,   0.330313,   0.143318,   1.62193,   -1.61167,   -0.749909,   1.72897,   0.332722,   -0.397443,   -1.5061,   -1.8401,   -1.69954,   1.85183,   3.21266,   1.58855,   -0.134339,   3.03707,   1.90422,   1.89492,   0.896055,   0.505357,   0.504491,   2.48799,   -0.0195389,   -0.359324,   3.1265,   0.729813,   -0.462166,   0.658854,   3.0718,   1.14115,   0.482358,   -1.70856,   2.24339,   -0.183906,   1.58289,   -0.464727,   0.828156,   -1.29483,   0.94111,   -0.154988,   -1.41235,   1.23551,   -0.0163243,   1.96128,   1.50838,   0.074505,   1.26635,   0.78061,   0.429321,   0.282356,   0.733974,   3.27142,   -1.11152,   -1.53347,   2.08893,   -1.1165,   -1.08778,   2.20677,   0.625069,   1.25285,   1.14361,   2.13902,   -1.44624,   1.91313,   1.35131,   -0.54242,   0.0930353,   -0.00395916,   -1.30979,   2.58687,   -0.108134,   0.176238,   2.57462,   -1.4481,   -0.370529,   0.737431,   0.0434251,   0.182732,   1.95483,   0.383301,   -0.501427,   0.413352,   -1.05596,   -0.789477,   -1.57058,   -0.541079,   -0.629878,   -0.388101,   -0.596181,   -1.56269,   1.61127,   -0.974619,   0.657275,   -0.164659,   1.25683,   0.43906,   -1.67909,   -0.475926,   -0.347827,   -0.162686,   0.724976,   1.40132,   2.46752,   -0.776612,   1.95958,   3.0806,   1.1599,   0.930841,   1.90771,   1.22125,   -0.388963,   1.55117,   1.94552,   -1.28208,   1.26578,   -2.46847,   0.256121,   -0.308766,   -1.30343,   0.443318,   1.14455,   0.564449,   -0.791313,   2.01193,   2.36051,   -0.219001,   -1.33125,   1.75063,   1.99016,   0.611425,   0.25704,   0.0177005,   -2.29396,   -1.30064,   -1.09289,   -1.14904,   0.562898,   0.417561,   1.75174,   1.77034,   0.327377,   0.780203,   1.62765,   0.996306,   -1.21466,   -1.31892,   -0.316939,   1.38053,   1.77509,   -1.21836,   0.921855,   0.342059,   -0.720169,   -0.0489885,   1.89164,   -0.718576,   0.966727,   1.20982,   0.432939,   1.97865,   0.189669,   0.402521,   -2.60331,   -0.48163,   0.612048,   0.143951,   0.150019,   0.140482,   2.13256,   0.788783,   0.82937,   -0.417994,   0.602195,   0.688793,   0.991777,   1.74104,   0.392452,   1.97726,   -0.605857,   -0.941777,   -0.306789,   0.98128,   1.6293,   -0.144457,   -0.636347,   -2.13689,   1.9183,   0.767051,   0.196158,   1.82617,   -2.14327,   2.26321,   1.79306,   0.83514,   0.424965,   1.93371,   -0.441106,   2.9036,   0.578882,   0.54742,   2.3148,   0.205643,   1.48523,   0.0773615,   0.917439,   1.61065,   1.01852,   -1.40839,   -0.636549,   -0.437628,   1.08115,   2.45224,   2.36365,   -0.849373,   -0.0690044,   -0.0561297,   -1.87224,   0.770108,   0.836116,   0.166863,   2.34146,   0.467317,   -0.299892,   1.82379,   -0.264927,   1.9868,   0.740764,   -0.481545,   -0.464522,   0.663787,   -0.278307,   -0.655509,   1.05058,   -0.253779,   -0.315195,   -0.453514,   -0.0913507,   -0.236444,   0.217139,   0.570419,   0.22635,   2.38319,   -0.646788,   -0.541801,   -0.956737,   -1.22133,   2.48195,   -1.25644,   -1.25525,   0.0213312,   0.576831,   1.14238,   0.178021,   -0.437331,   1.66009,   0.939586,   2.35029,   -1.59469,   1.05437,   1.8479,   -0.95017,   0.33873,   0.497559,   -0.773827,   1.73423,   1.30082,   1.44521,   -0.883242,   1.33984,   0.713126,   0.499596,   0.406151,   2.30366,   1.69081,   2.42171,   -0.644678,   0.340341,   -0.707323,   0.028267,   0.988085,   1.71466,   -0.583104,   0.450631,   1.17842,   0.101281,   0.445673,   -0.676183,   0.849173,   0.095416,   1.5569,   0.96694,   -0.454255,   -1.1007,   -2.2626,   -0.361266,   -1.24664,   -0.211135,   0.50281,   0.568694,   -0.163552,   1.44493,   -0.514323,   -1.81005,   0.579443,   0.53093,   0.362769,   0.120562,   0.310011,   0.119831,   0.367077,   1.05003,   0.822572,   -0.786232,   1.58978,   -1.18191,   -0.122847,   -0.0108107,   0.640054,   -0.658451,   1.24654,   2.52513,   1.3631,   0.648449,   -0.386697,   1.11236,   0.955,   0.85611,   0.98217,   -0.849822,   0.955925,   3.07502,   0.998507,   0.10692,   -0.04447,   1.52255,   0.978871,   -0.21451,   1.02856,   0.74671,   0.870629,   -1.03394,   0.0127823,   -0.648542,   -1.34141,   2.88018,   -0.132096,   -0.319593,   0.170581,   0.210474,   0.138525,   -0.102482,   -0.106998,   -0.832002,   0.379312,   0.0427166,   -0.523617,   -1.28519,   -0.515395,   0.50629,   -0.397784,   0.00858456,   -2.08857,   -1.12992,   -0.969793,   0.542077,   0.0475814,   0.365625,   -0.987327,   0.589917,   -1.11906,   -0.704181,   -0.436199,   -0.38234,   0.237474,   -0.536685,   -1.1669,   1.30213,   1.50769,   0.37628,   0.916139,   0.354946,   -0.324984,   0.953662,   -0.807864,   0.126086,   0.255914,   -0.0596456,   -0.274656,   0.458289,   -0.858904,   0.0987359,   -1.42958,   -0.514365,   -1.04381,   1.48747,   -0.0992355,   1.22911,   1.5846,   -0.819103,   -0.0786722,   0.0430288,   -1.25891,   -0.587588,   0.0615527,   0.115497,   -0.0392477,   0.41119,   -0.197634,   0.053867,   -0.439192,   -0.0551459,   -0.454589,   -0.00711279,   0.293686,   -1.2896,   0.338721,   -0.194584,   -0.852008,   1.09089,   -1.78432,   -1.52312,   -0.862169,   -0.681331,   -0.924081,   -0.554275,   -1.3331,   0.154912,   0.320575]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4.949861 GFLOPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(command=\"./execute.sh resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741bb59",
   "metadata": {},
   "source": [
    "### Using the refbackend of torch-mlir (jit compilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d52fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_module = compile_and_load_on_refbackend(compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97b3ac5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.9137e-02,  1.1446e-01, -1.7968e+00, -1.2343e+00, -8.1900e-01,\n",
       "          3.2396e-01, -2.1866e+00, -1.2877e+00, -1.9019e+00, -7.3148e-01,\n",
       "          7.1643e-01, -1.6698e+00, -1.4515e+00, -1.2659e+00, -1.5797e+00,\n",
       "         -1.0382e+00, -2.1478e-01, -2.0713e+00, -1.5538e+00, -1.2831e+00,\n",
       "         -5.8318e-01,  1.6193e+00, -3.0488e-02, -4.8139e-01, -1.1298e+00,\n",
       "         -3.6930e-01,  3.8818e-01,  5.7440e-02,  4.6316e-01, -2.7053e-01,\n",
       "         -1.4319e+00, -7.5139e-01, -4.1541e-01, -1.8500e+00, -4.2063e-01,\n",
       "         -1.1912e+00, -5.1930e-01, -1.9624e+00,  1.3662e+00, -1.1059e+00,\n",
       "         -7.7725e-01, -2.0080e-02,  1.3349e-01,  1.3197e+00, -2.2508e-01,\n",
       "          6.3489e-01, -1.1425e+00,  4.5811e-01, -8.9082e-01, -1.1984e+00,\n",
       "         -1.0954e+00,  1.4283e+00,  4.6136e-01, -4.3548e-01, -3.3565e-01,\n",
       "         -1.5134e+00, -9.2316e-01, -1.6104e+00, -1.0705e+00,  1.3485e+00,\n",
       "          2.2440e-01, -8.4757e-01,  1.3267e+00,  9.8154e-01,  6.4895e-01,\n",
       "         -2.2183e-01,  8.3419e-01, -1.0139e+00,  1.4669e+00, -4.4027e-01,\n",
       "         -1.0490e+00,  3.6617e-01, -7.9410e-01,  2.1867e+00, -1.7014e+00,\n",
       "          3.8230e-01, -1.0633e+00, -6.7834e-01,  1.7637e+00,  1.0572e-01,\n",
       "          1.9596e+00, -4.9507e-01, -3.2849e-01,  4.2277e-01, -1.8629e+00,\n",
       "         -9.5334e-01, -8.6545e-02, -4.1913e-01, -7.9765e-01, -6.8356e-01,\n",
       "         -1.6655e+00,  2.0318e-01,  6.5504e-01, -8.7574e-01, -1.2041e+00,\n",
       "          3.1359e-01, -5.4071e-01, -2.0863e+00,  1.3105e+00, -2.4725e-01,\n",
       "         -1.0014e+00,  2.5003e-01, -3.5498e-01,  2.1052e+00, -8.2748e-01,\n",
       "         -2.2430e+00, -9.4148e-01, -1.8201e+00, -2.0494e+00, -1.4745e+00,\n",
       "         -1.2197e+00,  3.5295e+00, -6.2990e-01,  9.1199e-02, -3.0923e-01,\n",
       "         -1.4608e+00, -7.3445e-01,  1.3739e+00, -1.0221e+00, -1.8682e+00,\n",
       "         -3.6143e-01, -1.2824e+00, -6.7404e-01, -1.5282e+00, -1.2835e+00,\n",
       "         -1.4214e+00,  7.1504e-01, -3.1045e-01,  1.7924e+00, -5.2723e-01,\n",
       "         -1.3796e+00, -1.1030e+00, -8.4108e-01, -4.1114e-01, -7.2738e-01,\n",
       "          8.3246e-01, -1.6222e+00, -8.6018e-01, -4.2163e-01, -1.2873e+00,\n",
       "         -7.1281e-01, -3.3673e-01, -9.1445e-03, -7.2685e-02, -9.9198e-01,\n",
       "         -5.0560e-01,  4.9048e-01,  1.4054e+00, -3.4878e-01,  3.7215e-01,\n",
       "         -7.1675e-01, -1.5458e-01,  8.8646e-01, -7.1024e-02, -1.4625e-01,\n",
       "         -8.8212e-01, -4.7917e-01, -3.2909e-01,  4.3415e-01, -3.8156e-01,\n",
       "         -3.5622e-01, -4.7552e-01, -6.5879e-01,  1.9298e-01,  5.6370e-01,\n",
       "          5.9261e-01,  6.5015e-01,  3.3835e-01,  4.4711e-01, -4.0328e-01,\n",
       "         -5.7533e-01, -7.8088e-01, -1.4635e-01, -4.1196e-01, -6.9653e-01,\n",
       "          4.2767e-01, -5.3691e-01, -4.7734e-01, -4.6046e-01, -3.2651e-01,\n",
       "         -6.5401e-01,  2.0136e-01, -8.2143e-01,  2.7485e-01, -3.4896e-01,\n",
       "         -1.1341e-01, -8.7412e-01,  2.7338e-01,  2.2087e-01, -2.3495e-01,\n",
       "          1.1783e+00, -2.6969e-01, -8.4415e-01, -7.6955e-01,  6.2568e-01,\n",
       "          2.2008e-01, -7.5163e-01, -6.9251e-01, -7.9160e-01, -6.8256e-02,\n",
       "         -2.9486e-01, -1.7693e+00,  6.5313e-02, -8.3293e-01, -8.3181e-01,\n",
       "         -3.8883e-01,  1.0319e+00,  1.1255e-01,  5.2640e-01,  1.3890e-01,\n",
       "          8.8303e-01, -7.5797e-01, -8.1138e-01, -9.7988e-01, -2.9219e-01,\n",
       "         -1.5626e-01,  3.5206e-01, -8.7496e-01, -1.1052e+00, -1.0364e+00,\n",
       "          7.8266e-01,  4.7428e-01, -1.0010e-01, -9.9586e-02,  1.0428e-01,\n",
       "         -7.3748e-01, -2.6968e-01, -4.0001e-01,  2.5801e-01, -5.1703e-01,\n",
       "         -1.2068e+00, -7.4788e-01,  1.9231e-01, -8.1077e-01, -1.0572e-01,\n",
       "         -2.2538e-01,  1.1943e-01, -7.5194e-01, -5.7978e-02, -7.6327e-02,\n",
       "         -2.2561e-02,  7.5653e-01, -1.0302e+00, -4.3123e-01, -7.0326e-01,\n",
       "         -3.5997e-01, -1.9849e-01, -4.4402e-02,  4.6587e-02, -6.5751e-02,\n",
       "         -2.7620e-01, -1.3066e+00,  1.0963e+00, -6.3939e-01, -4.4383e-01,\n",
       "         -1.0458e+00, -7.4001e-02, -2.0144e-01,  4.2804e-01, -4.9294e-01,\n",
       "         -4.9234e-01, -5.3631e-01,  5.8465e-01, -3.4944e-01,  2.4590e-01,\n",
       "         -4.8008e-01, -5.6149e-01, -3.5718e-01, -4.1738e-01, -1.8979e-01,\n",
       "         -1.4421e-01, -4.7171e-02,  4.8622e-01,  9.0277e-01,  1.0469e+00,\n",
       "          1.3819e+00,  5.6353e-01,  1.2107e+00,  1.0013e+00,  9.9233e-01,\n",
       "          5.6408e-01,  9.5280e-01,  2.8689e-02,  1.6313e+00,  9.9905e-02,\n",
       "          1.1656e+00, -9.2040e-01, -3.8474e-01,  4.4546e-01, -1.1219e+00,\n",
       "         -5.5180e-02,  1.6428e-01, -1.7936e-01,  1.1197e+00,  2.9721e-01,\n",
       "         -4.0459e-01, -2.9284e-01,  5.5242e-01,  6.1954e-01,  9.0297e-01,\n",
       "         -9.6591e-01, -4.6226e-01,  5.8417e-01, -3.3233e-01, -1.4020e+00,\n",
       "         -9.8971e-01, -2.8137e-01, -3.5019e-01, -7.2155e-01, -2.4212e+00,\n",
       "          6.9137e-01, -2.4344e-01, -2.4997e-01, -4.9728e-01,  1.8016e-01,\n",
       "         -7.0208e-01, -7.0976e-01, -1.1062e+00,  5.8095e-01,  3.0144e-01,\n",
       "         -1.7450e-01, -2.0151e+00, -1.7761e+00, -1.3495e+00, -1.3985e+00,\n",
       "         -1.7420e+00,  1.4142e-02, -8.0152e-01, -1.6386e+00, -1.6633e+00,\n",
       "          9.6960e-02,  6.9195e-01, -1.6853e-01, -2.2989e+00, -1.4693e+00,\n",
       "          1.3263e-01, -1.3813e+00, -8.7982e-02, -1.0109e+00, -1.9212e-01,\n",
       "         -1.2916e+00, -2.8748e-01, -3.7884e-01, -1.0630e+00, -1.2922e+00,\n",
       "         -4.9240e-01, -7.3856e-01, -6.8917e-01, -2.1651e+00, -6.2863e-01,\n",
       "          6.2011e-02,  8.0018e-01, -6.0782e-01,  4.2063e-01,  4.1657e-01,\n",
       "         -1.5792e+00,  1.2690e+00,  1.0095e+00,  1.3992e+00,  1.3111e+00,\n",
       "          4.2616e-01,  6.7466e-01,  1.9116e-01, -4.4642e-01,  3.3252e-01,\n",
       "         -9.3480e-01, -1.3908e+00, -2.3908e-01, -3.7912e-01, -2.8811e-01,\n",
       "         -3.4211e-01,  1.6614e-01, -7.6972e-01, -3.9412e-01, -4.8169e-01,\n",
       "         -6.1119e-01, -3.7043e-01,  2.7377e-02, -3.5915e-01, -1.2113e+00,\n",
       "         -8.9172e-01, -7.1293e-01, -5.8190e-01,  1.9753e-01, -6.4470e-01,\n",
       "          3.3336e-02, -3.2940e-01, -1.5643e-02, -8.0447e-01,  5.3135e-01,\n",
       "         -3.0172e-01, -9.7540e-01,  6.2660e-01, -2.8365e+00, -1.0067e+00,\n",
       "          6.8841e-01, -2.5539e+00, -1.7143e+00, -4.6985e-01,  2.5018e+00,\n",
       "          5.8443e-01, -7.3083e-01,  5.7662e-01, -5.9629e-01,  1.0317e+00,\n",
       "          8.3453e-01, -1.1740e+00, -1.2931e+00, -7.9261e-01,  2.1923e+00,\n",
       "         -1.6712e+00, -1.1683e+00,  9.2414e-02,  2.4659e+00, -6.8471e-01,\n",
       "         -2.5004e+00,  1.8859e+00, -7.8398e-01,  3.3357e+00,  2.2426e+00,\n",
       "          2.2914e-01,  2.3042e-01, -1.4629e-02,  7.3118e-01, -1.4039e+00,\n",
       "         -7.3490e-01,  1.2893e+00,  9.0140e-01, -7.8249e-01,  4.5099e-01,\n",
       "         -5.3137e-01, -1.5092e-01,  1.9249e+00, -6.4291e-01, -6.4990e-01,\n",
       "          3.4314e-01, -1.2938e+00, -4.4755e-01,  1.2359e+00,  1.2057e+00,\n",
       "         -6.8254e-02, -8.2368e-02, -8.6602e-01, -5.3396e-01, -1.0942e+00,\n",
       "         -7.6976e-01,  3.0671e+00,  4.2943e-01, -7.7776e-01, -2.6470e-03,\n",
       "         -5.0030e-01,  2.5627e+00, -8.4767e-01,  4.5491e-01, -2.1022e+00,\n",
       "         -8.0923e-01,  1.1497e+00,  1.5705e-01, -1.1990e+00,  4.2982e-01,\n",
       "         -7.9563e-01,  6.4697e-01,  2.2125e+00,  3.4438e-01,  1.2520e+00,\n",
       "          1.6449e+00, -1.4008e+00, -1.9928e+00, -1.4738e+00,  6.3666e-01,\n",
       "          1.1515e+00, -6.5920e-01, -2.2759e-01,  3.2760e+00, -8.8478e-01,\n",
       "         -1.6824e+00, -2.6889e+00,  5.5692e-01,  5.3319e-01, -5.2471e-01,\n",
       "         -8.4987e-01,  2.0403e+00,  1.3180e+00, -1.0281e+00,  1.9381e-02,\n",
       "          3.0638e-01,  5.8614e-01,  8.7920e-01,  1.7715e+00, -2.1069e+00,\n",
       "          6.7999e-01,  2.2857e+00,  2.0725e-01,  1.7107e+00,  1.9192e+00,\n",
       "         -5.8300e-01, -1.8784e-01, -1.9703e+00, -1.4303e+00,  3.8321e+00,\n",
       "         -7.0850e-01,  7.7494e-01,  1.2932e+00,  1.6125e+00,  1.0647e-01,\n",
       "         -2.8132e-01, -9.4210e-01,  2.3513e+00, -1.4464e-01, -2.6750e+00,\n",
       "          7.5041e-01, -1.1597e+00,  3.0237e+00,  5.6882e-01,  1.3561e+00,\n",
       "         -3.1194e-01,  9.8880e-01,  3.2301e-01, -2.4850e-01,  9.2857e-01,\n",
       "         -6.5356e-01,  9.4663e-01,  9.3051e-01,  5.6836e-01,  1.4294e-01,\n",
       "         -1.4507e+00,  4.1455e-01,  1.6771e+00, -9.3530e-02,  5.6068e-01,\n",
       "          2.2270e+00,  8.6688e-01, -7.1171e-03, -3.4681e-01,  2.2546e+00,\n",
       "         -1.2006e+00, -1.2632e+00, -4.1280e-01, -4.2133e-01,  9.8360e-01,\n",
       "          8.2727e-01,  6.2414e-01,  1.0821e+00,  7.6221e-01, -1.1229e+00,\n",
       "          9.7236e-01,  3.6126e-01, -1.9292e+00,  3.7656e-01,  2.0754e+00,\n",
       "          1.0764e+00,  2.3849e+00, -4.7310e-01,  1.5600e+00, -7.7742e-01,\n",
       "         -1.6711e+00,  2.6561e+00, -2.4844e-01,  9.6949e-01,  2.2555e+00,\n",
       "         -1.8119e+00, -5.0060e-01, -1.6838e+00,  1.2991e+00, -6.7307e-01,\n",
       "         -2.0724e+00, -1.8276e-01,  1.9738e+00, -6.7810e-01, -1.8189e+00,\n",
       "          4.4804e-01, -2.2888e+00, -9.0775e-03, -6.3471e-01,  1.3799e+00,\n",
       "         -1.6421e+00, -6.5993e-01,  2.2314e+00, -4.4599e-01, -9.5506e-01,\n",
       "         -2.1789e+00, -2.7452e+00, -1.8958e+00,  2.5002e+00,  3.3979e+00,\n",
       "          1.7930e+00, -2.6551e-01,  2.6639e+00,  1.8268e+00,  1.2778e+00,\n",
       "          1.3806e+00,  3.0474e-01,  3.6208e-01,  2.1400e+00,  8.9185e-02,\n",
       "         -1.0577e+00,  3.6192e+00, -2.5195e-01, -5.3709e-04, -1.2164e-02,\n",
       "          2.5895e+00,  1.1865e+00,  1.6631e+00, -1.9085e+00,  2.6173e+00,\n",
       "         -2.9602e-01,  1.0624e+00, -2.5587e-01, -1.1752e-01, -1.7098e+00,\n",
       "          1.3698e+00, -5.0393e-01, -9.1962e-01,  1.0532e+00, -3.8556e-01,\n",
       "          1.9615e+00,  1.2981e+00, -1.6031e-04,  1.1594e+00,  1.3718e+00,\n",
       "          1.1872e+00, -5.2268e-01,  4.1394e-01,  4.4229e+00, -1.5639e+00,\n",
       "         -8.4691e-01,  2.0330e+00, -1.4432e+00, -6.9855e-01,  2.3755e+00,\n",
       "          1.0496e+00,  1.8451e+00,  1.3324e+00,  2.4543e+00, -1.7141e+00,\n",
       "          1.9265e+00,  1.5339e+00, -1.0431e+00, -4.6131e-01,  4.6358e-01,\n",
       "         -1.9367e+00,  2.5352e+00, -4.4095e-01, -2.2268e-01,  2.7144e+00,\n",
       "         -1.1476e+00, -1.9659e-01,  8.9753e-01,  3.8406e-01, -5.6053e-02,\n",
       "          9.4256e-01,  1.1386e+00, -1.0381e+00,  1.3650e+00, -1.0931e+00,\n",
       "         -9.5962e-01, -1.3788e+00, -1.9990e-01, -1.0799e+00, -3.5175e-01,\n",
       "          1.0951e-01, -1.9182e+00,  2.1054e+00, -3.1669e-01,  5.6446e-01,\n",
       "         -4.8333e-01,  1.4286e+00,  9.4887e-02, -1.6460e+00, -8.9804e-01,\n",
       "         -7.9123e-01, -9.7618e-01,  1.1609e+00,  1.0264e+00,  2.5517e+00,\n",
       "          4.9464e-02,  1.9671e+00,  2.5284e+00,  1.3382e+00, -3.2269e-01,\n",
       "          2.3121e+00,  9.6904e-01, -2.9134e-01,  2.7646e+00,  2.4440e+00,\n",
       "         -1.7742e+00,  1.7003e+00, -2.2460e+00,  2.4294e-01,  5.9714e-01,\n",
       "         -1.1688e+00, -4.1351e-02,  1.0711e+00,  8.3898e-01, -8.3395e-01,\n",
       "          1.0552e+00,  1.8039e+00, -3.7036e-01, -1.4501e+00,  2.4589e+00,\n",
       "          1.5581e+00,  9.1646e-01,  1.3623e+00, -6.9055e-01, -2.5994e+00,\n",
       "         -7.7723e-01, -1.1390e+00, -1.3564e+00,  2.1629e+00, -6.9110e-01,\n",
       "          1.1380e+00,  1.0823e+00,  8.2704e-01,  1.0939e+00,  2.3358e+00,\n",
       "          1.1951e+00, -1.9673e+00, -1.7422e+00, -3.4944e-01,  8.3986e-01,\n",
       "          1.4572e+00, -1.7225e+00,  1.1983e+00,  8.1337e-01,  4.4138e-01,\n",
       "         -6.0274e-01,  2.3435e+00, -9.1950e-02,  1.2106e+00,  1.8956e+00,\n",
       "          1.1281e-01,  2.6774e+00, -3.8433e-01,  4.4615e-01, -2.4586e+00,\n",
       "         -7.9545e-01,  6.7095e-01, -2.6055e-01,  3.6025e-01,  2.2989e-01,\n",
       "          1.9240e+00,  1.1841e+00,  2.3769e-01,  2.2981e-01,  1.3572e+00,\n",
       "          6.8338e-01,  1.4842e+00,  2.0234e+00,  4.9779e-01,  2.1853e+00,\n",
       "         -2.3033e-01, -1.4224e+00, -8.2173e-01,  1.8110e+00,  7.6629e-01,\n",
       "         -3.1288e-01, -9.2537e-01, -2.3291e+00,  1.7023e+00,  6.2213e-01,\n",
       "          3.6009e-01,  1.6092e+00, -2.4815e+00,  3.1183e+00,  1.0248e+00,\n",
       "          6.0300e-01,  5.4041e-01,  1.9275e+00, -1.2525e+00,  2.5789e+00,\n",
       "          8.4572e-02,  1.5262e+00,  2.3252e+00, -1.7360e-01,  5.7965e-01,\n",
       "         -5.7062e-01,  6.8426e-01,  1.5091e+00,  2.7041e-01, -1.5575e+00,\n",
       "          1.1391e-01, -3.3817e-01,  2.0043e+00,  2.1164e+00,  2.3545e+00,\n",
       "         -1.7666e+00, -3.1594e-01, -1.3132e-01, -2.5674e+00,  1.8718e+00,\n",
       "          9.9077e-01, -4.1752e-01,  1.7813e+00,  3.5105e-02, -4.1140e-01,\n",
       "          1.5520e+00,  2.0679e-01,  9.1181e-01,  1.8841e+00, -3.2760e-01,\n",
       "         -1.1003e+00, -3.0345e-01, -6.5998e-01, -1.9406e-01,  1.7015e+00,\n",
       "         -4.9191e-02, -4.1786e-01, -5.5769e-01, -4.0718e-01, -7.2268e-01,\n",
       "          1.1936e+00,  1.3204e+00,  1.7629e+00,  3.4848e+00, -2.1731e-02,\n",
       "         -1.1756e+00, -1.3610e+00, -7.4850e-01,  2.2494e+00, -1.6354e+00,\n",
       "         -1.8949e+00,  6.8919e-01,  6.0209e-01,  7.7399e-01, -5.3481e-01,\n",
       "         -1.0286e+00,  1.5340e+00,  9.4239e-01,  1.2488e+00, -2.4921e+00,\n",
       "          8.3279e-01,  2.6768e+00, -1.0139e+00,  7.5823e-01,  7.6455e-01,\n",
       "         -1.7233e+00,  1.2736e+00,  1.4504e-01,  1.8338e+00, -1.1552e+00,\n",
       "          6.3253e-01,  1.2164e+00,  6.2187e-01, -5.0089e-01,  2.4650e+00,\n",
       "          1.2423e+00,  3.2745e+00, -6.6663e-01,  2.9125e-01, -1.3240e+00,\n",
       "         -7.2379e-01,  9.7687e-01,  9.7483e-01, -8.7951e-01,  5.7783e-01,\n",
       "          1.0620e+00,  4.2585e-01,  3.7867e-01, -7.2237e-01,  4.6414e-01,\n",
       "          7.7815e-01,  1.4069e+00,  1.3072e+00, -9.5347e-01, -1.2602e+00,\n",
       "         -2.1281e+00, -9.2489e-01, -5.4484e-01, -3.5153e-01,  1.3650e+00,\n",
       "         -2.8391e-01,  7.3903e-01,  2.0880e+00, -1.0144e+00, -2.2068e+00,\n",
       "          6.4235e-01,  2.2301e-01, -2.4208e-01,  5.1390e-01, -3.3704e-01,\n",
       "         -3.0348e-01,  1.1484e+00,  3.9708e-01,  1.2999e+00, -9.7054e-01,\n",
       "          2.6387e+00, -1.9616e+00, -5.3803e-01, -5.8058e-01, -2.8935e-01,\n",
       "         -1.2451e+00,  1.5961e+00,  3.0976e+00,  1.6709e+00,  5.8443e-01,\n",
       "          1.7185e-01,  7.3836e-01,  7.2738e-01,  3.8646e-03,  8.3568e-01,\n",
       "         -1.0136e+00,  2.5430e+00,  3.2376e+00,  5.7196e-01,  3.4811e-01,\n",
       "          4.4878e-01,  2.2305e+00,  2.9298e-01, -2.5760e-01,  4.9370e-01,\n",
       "          4.8142e-01,  2.7368e-01, -1.9375e-01, -1.6452e-01,  8.1183e-01,\n",
       "         -1.6327e+00,  1.0103e+00, -2.2403e-01, -5.9727e-01, -2.7428e-02,\n",
       "          3.1328e-01,  9.7208e-01, -5.9249e-01, -1.7367e-01, -1.6532e+00,\n",
       "          1.0890e+00,  3.9624e-01, -1.2866e-01, -1.9037e+00, -7.7702e-01,\n",
       "          7.4127e-01, -5.8078e-01, -3.7049e-01, -2.8162e+00, -1.3003e+00,\n",
       "         -1.2620e+00,  1.4666e-01, -3.5499e-01,  2.5450e-01, -1.1414e+00,\n",
       "          1.1944e+00, -1.1381e+00, -3.8452e-01, -8.1482e-01, -1.0100e+00,\n",
       "          4.5429e-01, -4.2740e-01, -1.9397e+00,  7.4178e-01,  1.1876e+00,\n",
       "         -4.4543e-01,  1.0716e-01,  1.8230e-01, -1.1872e+00,  1.2492e-01,\n",
       "         -1.2264e+00,  1.4666e-02, -7.2432e-01, -1.0472e+00, -1.9104e-01,\n",
       "          2.9108e-01, -1.0063e+00,  1.2624e-01, -1.7337e+00, -5.0851e-01,\n",
       "         -1.6184e+00,  6.9612e-01, -4.5302e-01,  4.2452e-01,  1.8650e+00,\n",
       "         -9.0072e-01, -6.3056e-01, -8.6350e-01, -2.2329e+00,  2.3365e-01,\n",
       "          2.2275e-01,  4.3871e-01,  4.2248e-01,  8.1404e-01, -2.5820e-01,\n",
       "          4.3572e-01, -4.8871e-01, -1.0813e+00, -1.3633e+00, -7.3145e-01,\n",
       "         -1.0284e-01, -1.1898e+00, -1.8862e-02, -7.3265e-01, -1.0590e+00,\n",
       "          2.4528e-01, -2.1077e+00, -2.1840e+00, -6.8829e-01, -7.9754e-01,\n",
       "         -1.8375e-01, -5.9645e-01, -1.5152e+00,  1.7243e-01,  1.8253e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.from_numpy(jit_module.forward(example_input.numpy()))\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172dd176",
   "metadata": {},
   "source": [
    "### Extra: Get PyTorch graph IR\n",
    "https://pytorch.org/docs/stable/fx.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3dfcc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : torch.Tensor [num_users=1] = placeholder[target=x]\n",
      "    %conv1 : [num_users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n",
      "    %bn1 : [num_users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n",
      "    %relu : [num_users=1] = call_module[target=relu](args = (%bn1,), kwargs = {})\n",
      "    %maxpool : [num_users=2] = call_module[target=maxpool](args = (%relu,), kwargs = {})\n",
      "    %layer1_0_conv1 : [num_users=1] = call_module[target=layer1.0.conv1](args = (%maxpool,), kwargs = {})\n",
      "    %layer1_0_bn1 : [num_users=1] = call_module[target=layer1.0.bn1](args = (%layer1_0_conv1,), kwargs = {})\n",
      "    %layer1_0_relu : [num_users=1] = call_module[target=layer1.0.relu](args = (%layer1_0_bn1,), kwargs = {})\n",
      "    %layer1_0_conv2 : [num_users=1] = call_module[target=layer1.0.conv2](args = (%layer1_0_relu,), kwargs = {})\n",
      "    %layer1_0_bn2 : [num_users=1] = call_module[target=layer1.0.bn2](args = (%layer1_0_conv2,), kwargs = {})\n",
      "    %add : [num_users=1] = call_function[target=operator.add](args = (%layer1_0_bn2, %maxpool), kwargs = {})\n",
      "    %layer1_0_relu_1 : [num_users=2] = call_module[target=layer1.0.relu](args = (%add,), kwargs = {})\n",
      "    %layer1_1_conv1 : [num_users=1] = call_module[target=layer1.1.conv1](args = (%layer1_0_relu_1,), kwargs = {})\n",
      "    %layer1_1_bn1 : [num_users=1] = call_module[target=layer1.1.bn1](args = (%layer1_1_conv1,), kwargs = {})\n",
      "    %layer1_1_relu : [num_users=1] = call_module[target=layer1.1.relu](args = (%layer1_1_bn1,), kwargs = {})\n",
      "    %layer1_1_conv2 : [num_users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_relu,), kwargs = {})\n",
      "    %layer1_1_bn2 : [num_users=1] = call_module[target=layer1.1.bn2](args = (%layer1_1_conv2,), kwargs = {})\n",
      "    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%layer1_1_bn2, %layer1_0_relu_1), kwargs = {})\n",
      "    %layer1_1_relu_1 : [num_users=2] = call_module[target=layer1.1.relu](args = (%add_1,), kwargs = {})\n",
      "    %layer2_0_conv1 : [num_users=1] = call_module[target=layer2.0.conv1](args = (%layer1_1_relu_1,), kwargs = {})\n",
      "    %layer2_0_bn1 : [num_users=1] = call_module[target=layer2.0.bn1](args = (%layer2_0_conv1,), kwargs = {})\n",
      "    %layer2_0_relu : [num_users=1] = call_module[target=layer2.0.relu](args = (%layer2_0_bn1,), kwargs = {})\n",
      "    %layer2_0_conv2 : [num_users=1] = call_module[target=layer2.0.conv2](args = (%layer2_0_relu,), kwargs = {})\n",
      "    %layer2_0_bn2 : [num_users=1] = call_module[target=layer2.0.bn2](args = (%layer2_0_conv2,), kwargs = {})\n",
      "    %layer2_0_downsample_0 : [num_users=1] = call_module[target=layer2.0.downsample.0](args = (%layer1_1_relu_1,), kwargs = {})\n",
      "    %layer2_0_downsample_1 : [num_users=1] = call_module[target=layer2.0.downsample.1](args = (%layer2_0_downsample_0,), kwargs = {})\n",
      "    %add_2 : [num_users=1] = call_function[target=operator.add](args = (%layer2_0_bn2, %layer2_0_downsample_1), kwargs = {})\n",
      "    %layer2_0_relu_1 : [num_users=2] = call_module[target=layer2.0.relu](args = (%add_2,), kwargs = {})\n",
      "    %layer2_1_conv1 : [num_users=1] = call_module[target=layer2.1.conv1](args = (%layer2_0_relu_1,), kwargs = {})\n",
      "    %layer2_1_bn1 : [num_users=1] = call_module[target=layer2.1.bn1](args = (%layer2_1_conv1,), kwargs = {})\n",
      "    %layer2_1_relu : [num_users=1] = call_module[target=layer2.1.relu](args = (%layer2_1_bn1,), kwargs = {})\n",
      "    %layer2_1_conv2 : [num_users=1] = call_module[target=layer2.1.conv2](args = (%layer2_1_relu,), kwargs = {})\n",
      "    %layer2_1_bn2 : [num_users=1] = call_module[target=layer2.1.bn2](args = (%layer2_1_conv2,), kwargs = {})\n",
      "    %add_3 : [num_users=1] = call_function[target=operator.add](args = (%layer2_1_bn2, %layer2_0_relu_1), kwargs = {})\n",
      "    %layer2_1_relu_1 : [num_users=2] = call_module[target=layer2.1.relu](args = (%add_3,), kwargs = {})\n",
      "    %layer3_0_conv1 : [num_users=1] = call_module[target=layer3.0.conv1](args = (%layer2_1_relu_1,), kwargs = {})\n",
      "    %layer3_0_bn1 : [num_users=1] = call_module[target=layer3.0.bn1](args = (%layer3_0_conv1,), kwargs = {})\n",
      "    %layer3_0_relu : [num_users=1] = call_module[target=layer3.0.relu](args = (%layer3_0_bn1,), kwargs = {})\n",
      "    %layer3_0_conv2 : [num_users=1] = call_module[target=layer3.0.conv2](args = (%layer3_0_relu,), kwargs = {})\n",
      "    %layer3_0_bn2 : [num_users=1] = call_module[target=layer3.0.bn2](args = (%layer3_0_conv2,), kwargs = {})\n",
      "    %layer3_0_downsample_0 : [num_users=1] = call_module[target=layer3.0.downsample.0](args = (%layer2_1_relu_1,), kwargs = {})\n",
      "    %layer3_0_downsample_1 : [num_users=1] = call_module[target=layer3.0.downsample.1](args = (%layer3_0_downsample_0,), kwargs = {})\n",
      "    %add_4 : [num_users=1] = call_function[target=operator.add](args = (%layer3_0_bn2, %layer3_0_downsample_1), kwargs = {})\n",
      "    %layer3_0_relu_1 : [num_users=2] = call_module[target=layer3.0.relu](args = (%add_4,), kwargs = {})\n",
      "    %layer3_1_conv1 : [num_users=1] = call_module[target=layer3.1.conv1](args = (%layer3_0_relu_1,), kwargs = {})\n",
      "    %layer3_1_bn1 : [num_users=1] = call_module[target=layer3.1.bn1](args = (%layer3_1_conv1,), kwargs = {})\n",
      "    %layer3_1_relu : [num_users=1] = call_module[target=layer3.1.relu](args = (%layer3_1_bn1,), kwargs = {})\n",
      "    %layer3_1_conv2 : [num_users=1] = call_module[target=layer3.1.conv2](args = (%layer3_1_relu,), kwargs = {})\n",
      "    %layer3_1_bn2 : [num_users=1] = call_module[target=layer3.1.bn2](args = (%layer3_1_conv2,), kwargs = {})\n",
      "    %add_5 : [num_users=1] = call_function[target=operator.add](args = (%layer3_1_bn2, %layer3_0_relu_1), kwargs = {})\n",
      "    %layer3_1_relu_1 : [num_users=2] = call_module[target=layer3.1.relu](args = (%add_5,), kwargs = {})\n",
      "    %layer4_0_conv1 : [num_users=1] = call_module[target=layer4.0.conv1](args = (%layer3_1_relu_1,), kwargs = {})\n",
      "    %layer4_0_bn1 : [num_users=1] = call_module[target=layer4.0.bn1](args = (%layer4_0_conv1,), kwargs = {})\n",
      "    %layer4_0_relu : [num_users=1] = call_module[target=layer4.0.relu](args = (%layer4_0_bn1,), kwargs = {})\n",
      "    %layer4_0_conv2 : [num_users=1] = call_module[target=layer4.0.conv2](args = (%layer4_0_relu,), kwargs = {})\n",
      "    %layer4_0_bn2 : [num_users=1] = call_module[target=layer4.0.bn2](args = (%layer4_0_conv2,), kwargs = {})\n",
      "    %layer4_0_downsample_0 : [num_users=1] = call_module[target=layer4.0.downsample.0](args = (%layer3_1_relu_1,), kwargs = {})\n",
      "    %layer4_0_downsample_1 : [num_users=1] = call_module[target=layer4.0.downsample.1](args = (%layer4_0_downsample_0,), kwargs = {})\n",
      "    %add_6 : [num_users=1] = call_function[target=operator.add](args = (%layer4_0_bn2, %layer4_0_downsample_1), kwargs = {})\n",
      "    %layer4_0_relu_1 : [num_users=2] = call_module[target=layer4.0.relu](args = (%add_6,), kwargs = {})\n",
      "    %layer4_1_conv1 : [num_users=1] = call_module[target=layer4.1.conv1](args = (%layer4_0_relu_1,), kwargs = {})\n",
      "    %layer4_1_bn1 : [num_users=1] = call_module[target=layer4.1.bn1](args = (%layer4_1_conv1,), kwargs = {})\n",
      "    %layer4_1_relu : [num_users=1] = call_module[target=layer4.1.relu](args = (%layer4_1_bn1,), kwargs = {})\n",
      "    %layer4_1_conv2 : [num_users=1] = call_module[target=layer4.1.conv2](args = (%layer4_1_relu,), kwargs = {})\n",
      "    %layer4_1_bn2 : [num_users=1] = call_module[target=layer4.1.bn2](args = (%layer4_1_conv2,), kwargs = {})\n",
      "    %add_7 : [num_users=1] = call_function[target=operator.add](args = (%layer4_1_bn2, %layer4_0_relu_1), kwargs = {})\n",
      "    %layer4_1_relu_1 : [num_users=1] = call_module[target=layer4.1.relu](args = (%add_7,), kwargs = {})\n",
      "    %avgpool : [num_users=1] = call_module[target=avgpool](args = (%layer4_1_relu_1,), kwargs = {})\n",
      "    %flatten : [num_users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})\n",
      "    %fc : [num_users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})\n",
      "    return fc\n"
     ]
    }
   ],
   "source": [
    "from torch.fx import symbolic_trace\n",
    "\n",
    "# Symbolic tracing frontend - captures the semantics of the model \n",
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(model)\n",
    "\n",
    "# High level IR \n",
    "print(symbolic_traced.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f459a",
   "metadata": {},
   "source": [
    "You can get the same MLIR rep using the pytorch graph IR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "472dcc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>\n",
      "#map1 = affine_map<(d0, d1, d2, d3) -> (d1)>\n",
      "#map2 = affine_map<(d0, d1, d2, d3) -> (0, d1, d2, d3)>\n",
      "#map3 = affine_map<(d0, d1) -> (d0, d1)>\n",
      "#map4 = affine_map<(d0, d1) -> (d1, d0)>\n",
      "#map5 = affine_map<(d0, d1) -> (0, d1)>\n",
      "#map6 = affine_map<(d0, d1) -> (d1)>\n",
      "module attributes {torch.debug_module_name = \"ResNet\"} {\n",
      "  ml_program.global private mutable @global_seed(dense<0> : tensor<i64>) : tensor<i64>\n",
      "  func.func @forward(%arg0: tensor<1x3x224x224xf32>) -> tensor<1x1000xf32> {\n",
      "    %false = arith.constant false\n",
      "    %cst = arith.constant dense_resource<__elided__> : tensor<1000xf32>\n",
      "    %cst_0 = arith.constant dense_resource<__elided__> : tensor<1000x512xf32>\n",
      "    %cst_1 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_2 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_3 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_4 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_5 = arith.constant dense_resource<__elided__> : tensor<512x512x3x3xf32>\n",
      "    %cst_6 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_7 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_8 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_9 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_10 = arith.constant dense_resource<__elided__> : tensor<512x512x3x3xf32>\n",
      "    %cst_11 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_12 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_13 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_14 = arith.constant dense_resource<__elided__> : tensor<512x256x1x1xf32>\n",
      "    %cst_15 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_16 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_17 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_18 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_19 = arith.constant dense_resource<__elided__> : tensor<512x512x3x3xf32>\n",
      "    %cst_20 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_21 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_22 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_23 = arith.constant dense_resource<__elided__> : tensor<512xf32>\n",
      "    %cst_24 = arith.constant dense_resource<__elided__> : tensor<512x256x3x3xf32>\n",
      "    %cst_25 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_26 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_27 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_28 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_29 = arith.constant dense_resource<__elided__> : tensor<256x256x3x3xf32>\n",
      "    %cst_30 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_31 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_32 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_33 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_34 = arith.constant dense_resource<__elided__> : tensor<256x256x3x3xf32>\n",
      "    %cst_35 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_36 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_37 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_38 = arith.constant dense_resource<__elided__> : tensor<256x128x1x1xf32>\n",
      "    %cst_39 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_40 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_41 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_42 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_43 = arith.constant dense_resource<__elided__> : tensor<256x256x3x3xf32>\n",
      "    %cst_44 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_45 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_46 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_47 = arith.constant dense_resource<__elided__> : tensor<256xf32>\n",
      "    %cst_48 = arith.constant dense_resource<__elided__> : tensor<256x128x3x3xf32>\n",
      "    %cst_49 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_50 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_51 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_52 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_53 = arith.constant dense_resource<__elided__> : tensor<128x128x3x3xf32>\n",
      "    %cst_54 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_55 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_56 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_57 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_58 = arith.constant dense_resource<__elided__> : tensor<128x128x3x3xf32>\n",
      "    %cst_59 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_60 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_61 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_62 = arith.constant dense_resource<__elided__> : tensor<128x64x1x1xf32>\n",
      "    %cst_63 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_64 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_65 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_66 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_67 = arith.constant dense_resource<__elided__> : tensor<128x128x3x3xf32>\n",
      "    %cst_68 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_69 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_70 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_71 = arith.constant dense_resource<__elided__> : tensor<128xf32>\n",
      "    %cst_72 = arith.constant dense_resource<__elided__> : tensor<128x64x3x3xf32>\n",
      "    %cst_73 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_74 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_75 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_76 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_77 = arith.constant dense_resource<__elided__> : tensor<64x64x3x3xf32>\n",
      "    %cst_78 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_79 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_80 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_81 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_82 = arith.constant dense_resource<__elided__> : tensor<64x64x3x3xf32>\n",
      "    %cst_83 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_84 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_85 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_86 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_87 = arith.constant dense_resource<__elided__> : tensor<64x64x3x3xf32>\n",
      "    %cst_88 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_89 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_90 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_91 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_92 = arith.constant dense_resource<__elided__> : tensor<64x64x3x3xf32>\n",
      "    %cst_93 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_94 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_95 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_96 = arith.constant dense_resource<__elided__> : tensor<64xf32>\n",
      "    %cst_97 = arith.constant dense_resource<__elided__> : tensor<64x3x7x7xf32>\n",
      "    %cst_98 = arith.constant 0.000000e+00 : f32\n",
      "    %cst_99 = arith.constant 0xFF800000 : f32\n",
      "    %cst_100 = arith.constant 1.000000e-05 : f64\n",
      "    %c3 = arith.constant 3 : index\n",
      "    %c0 = arith.constant 0 : index\n",
      "    %c1 = arith.constant 1 : index\n",
      "    %cst_101 = arith.constant 4.900000e+01 : f32\n",
      "    %padded = tensor.pad %arg0 low[0, 0, 3, 3] high[0, 0, 3, 3] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x3x224x224xf32> to tensor<1x3x230x230xf32>\n",
      "    %0 = tensor.empty() : tensor<1x64x112x112xf32>\n",
      "    %1 = linalg.fill ins(%cst_98 : f32) outs(%0 : tensor<1x64x112x112xf32>) -> tensor<1x64x112x112xf32>\n",
      "    %2 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%padded, %cst_97 : tensor<1x3x230x230xf32>, tensor<64x3x7x7xf32>) outs(%1 : tensor<1x64x112x112xf32>) -> tensor<1x64x112x112xf32>\n",
      "    %3 = arith.cmpi eq, %false, %false : i1\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %4 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%2, %cst_94, %cst_93, %cst_96, %cst_95 : tensor<1x64x112x112xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) outs(%2 : tensor<1x64x112x112xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x64x112x112xf32>\n",
      "    %5 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%4 : tensor<1x64x112x112xf32>) outs(%0 : tensor<1x64x112x112xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x64x112x112xf32>\n",
      "    %padded_102 = tensor.pad %5 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_99 : f32\n",
      "    } : tensor<1x64x112x112xf32> to tensor<1x64x114x114xf32>\n",
      "    %6 = tensor.empty() : tensor<1x64x56x56xf32>\n",
      "    %7 = linalg.fill ins(%cst_99 : f32) outs(%6 : tensor<1x64x56x56xf32>) -> tensor<1x64x56x56xf32>\n",
      "    %8 = tensor.empty() : tensor<3x3xf32>\n",
      "    %9 = linalg.pooling_nchw_max {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%padded_102, %8 : tensor<1x64x114x114xf32>, tensor<3x3xf32>) outs(%7 : tensor<1x64x56x56xf32>) -> tensor<1x64x56x56xf32>\n",
      "    %padded_103 = tensor.pad %9 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x64x56x56xf32> to tensor<1x64x58x58xf32>\n",
      "    %10 = linalg.fill ins(%cst_98 : f32) outs(%6 : tensor<1x64x56x56xf32>) -> tensor<1x64x56x56xf32>\n",
      "    %11 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_103, %cst_92 : tensor<1x64x58x58xf32>, tensor<64x64x3x3xf32>) outs(%10 : tensor<1x64x56x56xf32>) -> tensor<1x64x56x56xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %12 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%11, %cst_89, %cst_88, %cst_91, %cst_90 : tensor<1x64x56x56xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) outs(%11 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %13 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%12 : tensor<1x64x56x56xf32>) outs(%6 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %padded_104 = tensor.pad %13 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x64x56x56xf32> to tensor<1x64x58x58xf32>\n",
      "    %14 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_104, %cst_87 : tensor<1x64x58x58xf32>, tensor<64x64x3x3xf32>) outs(%10 : tensor<1x64x56x56xf32>) -> tensor<1x64x56x56xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %15 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%14, %cst_84, %cst_83, %cst_86, %cst_85 : tensor<1x64x56x56xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) outs(%14 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %16 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%15, %9 : tensor<1x64x56x56xf32>, tensor<1x64x56x56xf32>) outs(%6 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %17 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%16 : tensor<1x64x56x56xf32>) outs(%6 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %padded_105 = tensor.pad %17 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x64x56x56xf32> to tensor<1x64x58x58xf32>\n",
      "    %18 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_105, %cst_82 : tensor<1x64x58x58xf32>, tensor<64x64x3x3xf32>) outs(%10 : tensor<1x64x56x56xf32>) -> tensor<1x64x56x56xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %19 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%18, %cst_79, %cst_78, %cst_81, %cst_80 : tensor<1x64x56x56xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) outs(%18 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %20 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%19 : tensor<1x64x56x56xf32>) outs(%6 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %padded_106 = tensor.pad %20 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x64x56x56xf32> to tensor<1x64x58x58xf32>\n",
      "    %21 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_106, %cst_77 : tensor<1x64x58x58xf32>, tensor<64x64x3x3xf32>) outs(%10 : tensor<1x64x56x56xf32>) -> tensor<1x64x56x56xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %22 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%21, %cst_74, %cst_73, %cst_76, %cst_75 : tensor<1x64x56x56xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>, tensor<64xf32>) outs(%21 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %23 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%22, %17 : tensor<1x64x56x56xf32>, tensor<1x64x56x56xf32>) outs(%6 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %24 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%23 : tensor<1x64x56x56xf32>) outs(%6 : tensor<1x64x56x56xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x64x56x56xf32>\n",
      "    %padded_107 = tensor.pad %24 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x64x56x56xf32> to tensor<1x64x58x58xf32>\n",
      "    %25 = tensor.empty() : tensor<1x128x28x28xf32>\n",
      "    %26 = linalg.fill ins(%cst_98 : f32) outs(%25 : tensor<1x128x28x28xf32>) -> tensor<1x128x28x28xf32>\n",
      "    %27 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%padded_107, %cst_72 : tensor<1x64x58x58xf32>, tensor<128x64x3x3xf32>) outs(%26 : tensor<1x128x28x28xf32>) -> tensor<1x128x28x28xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %28 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%27, %cst_69, %cst_68, %cst_71, %cst_70 : tensor<1x128x28x28xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) outs(%27 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %29 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%28 : tensor<1x128x28x28xf32>) outs(%25 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %padded_108 = tensor.pad %29 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x128x28x28xf32> to tensor<1x128x30x30xf32>\n",
      "    %30 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_108, %cst_67 : tensor<1x128x30x30xf32>, tensor<128x128x3x3xf32>) outs(%26 : tensor<1x128x28x28xf32>) -> tensor<1x128x28x28xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %31 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%30, %cst_64, %cst_63, %cst_66, %cst_65 : tensor<1x128x28x28xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) outs(%30 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %32 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%24, %cst_62 : tensor<1x64x56x56xf32>, tensor<128x64x1x1xf32>) outs(%26 : tensor<1x128x28x28xf32>) -> tensor<1x128x28x28xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %33 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%32, %cst_59, %cst_63, %cst_61, %cst_60 : tensor<1x128x28x28xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) outs(%32 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %34 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%31, %33 : tensor<1x128x28x28xf32>, tensor<1x128x28x28xf32>) outs(%25 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %35 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%34 : tensor<1x128x28x28xf32>) outs(%25 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %padded_109 = tensor.pad %35 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x128x28x28xf32> to tensor<1x128x30x30xf32>\n",
      "    %36 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_109, %cst_58 : tensor<1x128x30x30xf32>, tensor<128x128x3x3xf32>) outs(%26 : tensor<1x128x28x28xf32>) -> tensor<1x128x28x28xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %37 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%36, %cst_55, %cst_54, %cst_57, %cst_56 : tensor<1x128x28x28xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) outs(%36 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %38 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%37 : tensor<1x128x28x28xf32>) outs(%25 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %padded_110 = tensor.pad %38 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x128x28x28xf32> to tensor<1x128x30x30xf32>\n",
      "    %39 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_110, %cst_53 : tensor<1x128x30x30xf32>, tensor<128x128x3x3xf32>) outs(%26 : tensor<1x128x28x28xf32>) -> tensor<1x128x28x28xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %40 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%39, %cst_50, %cst_49, %cst_52, %cst_51 : tensor<1x128x28x28xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>, tensor<128xf32>) outs(%39 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %41 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%40, %35 : tensor<1x128x28x28xf32>, tensor<1x128x28x28xf32>) outs(%25 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %42 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%41 : tensor<1x128x28x28xf32>) outs(%25 : tensor<1x128x28x28xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x128x28x28xf32>\n",
      "    %padded_111 = tensor.pad %42 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x128x28x28xf32> to tensor<1x128x30x30xf32>\n",
      "    %43 = tensor.empty() : tensor<1x256x14x14xf32>\n",
      "    %44 = linalg.fill ins(%cst_98 : f32) outs(%43 : tensor<1x256x14x14xf32>) -> tensor<1x256x14x14xf32>\n",
      "    %45 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%padded_111, %cst_48 : tensor<1x128x30x30xf32>, tensor<256x128x3x3xf32>) outs(%44 : tensor<1x256x14x14xf32>) -> tensor<1x256x14x14xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %46 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%45, %cst_45, %cst_44, %cst_47, %cst_46 : tensor<1x256x14x14xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%45 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %47 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%46 : tensor<1x256x14x14xf32>) outs(%43 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %padded_112 = tensor.pad %47 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x256x14x14xf32> to tensor<1x256x16x16xf32>\n",
      "    %48 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_112, %cst_43 : tensor<1x256x16x16xf32>, tensor<256x256x3x3xf32>) outs(%44 : tensor<1x256x14x14xf32>) -> tensor<1x256x14x14xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %49 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%48, %cst_40, %cst_39, %cst_42, %cst_41 : tensor<1x256x14x14xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%48 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %50 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%42, %cst_38 : tensor<1x128x28x28xf32>, tensor<256x128x1x1xf32>) outs(%44 : tensor<1x256x14x14xf32>) -> tensor<1x256x14x14xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %51 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%50, %cst_35, %cst_39, %cst_37, %cst_36 : tensor<1x256x14x14xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%50 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %52 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%49, %51 : tensor<1x256x14x14xf32>, tensor<1x256x14x14xf32>) outs(%43 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %53 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%52 : tensor<1x256x14x14xf32>) outs(%43 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %padded_113 = tensor.pad %53 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x256x14x14xf32> to tensor<1x256x16x16xf32>\n",
      "    %54 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_113, %cst_34 : tensor<1x256x16x16xf32>, tensor<256x256x3x3xf32>) outs(%44 : tensor<1x256x14x14xf32>) -> tensor<1x256x14x14xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %55 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%54, %cst_31, %cst_30, %cst_33, %cst_32 : tensor<1x256x14x14xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%54 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %56 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%55 : tensor<1x256x14x14xf32>) outs(%43 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %padded_114 = tensor.pad %56 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x256x14x14xf32> to tensor<1x256x16x16xf32>\n",
      "    %57 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_114, %cst_29 : tensor<1x256x16x16xf32>, tensor<256x256x3x3xf32>) outs(%44 : tensor<1x256x14x14xf32>) -> tensor<1x256x14x14xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %58 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%57, %cst_26, %cst_25, %cst_28, %cst_27 : tensor<1x256x14x14xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>, tensor<256xf32>) outs(%57 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %59 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%58, %53 : tensor<1x256x14x14xf32>, tensor<1x256x14x14xf32>) outs(%43 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %60 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%59 : tensor<1x256x14x14xf32>) outs(%43 : tensor<1x256x14x14xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x256x14x14xf32>\n",
      "    %padded_115 = tensor.pad %60 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x256x14x14xf32> to tensor<1x256x16x16xf32>\n",
      "    %61 = tensor.empty() : tensor<1x512x7x7xf32>\n",
      "    %62 = linalg.fill ins(%cst_98 : f32) outs(%61 : tensor<1x512x7x7xf32>) -> tensor<1x512x7x7xf32>\n",
      "    %63 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%padded_115, %cst_24 : tensor<1x256x16x16xf32>, tensor<512x256x3x3xf32>) outs(%62 : tensor<1x512x7x7xf32>) -> tensor<1x512x7x7xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %64 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%63, %cst_21, %cst_20, %cst_23, %cst_22 : tensor<1x512x7x7xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) outs(%63 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %65 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%64 : tensor<1x512x7x7xf32>) outs(%61 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %padded_116 = tensor.pad %65 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x512x7x7xf32> to tensor<1x512x9x9xf32>\n",
      "    %66 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_116, %cst_19 : tensor<1x512x9x9xf32>, tensor<512x512x3x3xf32>) outs(%62 : tensor<1x512x7x7xf32>) -> tensor<1x512x7x7xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %67 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%66, %cst_16, %cst_15, %cst_18, %cst_17 : tensor<1x512x7x7xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) outs(%66 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %68 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<2> : vector<2xi64>} ins(%60, %cst_14 : tensor<1x256x14x14xf32>, tensor<512x256x1x1xf32>) outs(%62 : tensor<1x512x7x7xf32>) -> tensor<1x512x7x7xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %69 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%68, %cst_11, %cst_15, %cst_13, %cst_12 : tensor<1x512x7x7xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) outs(%68 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %70 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%67, %69 : tensor<1x512x7x7xf32>, tensor<1x512x7x7xf32>) outs(%61 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %71 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%70 : tensor<1x512x7x7xf32>) outs(%61 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %padded_117 = tensor.pad %71 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x512x7x7xf32> to tensor<1x512x9x9xf32>\n",
      "    %72 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_117, %cst_10 : tensor<1x512x9x9xf32>, tensor<512x512x3x3xf32>) outs(%62 : tensor<1x512x7x7xf32>) -> tensor<1x512x7x7xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %73 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%72, %cst_7, %cst_6, %cst_9, %cst_8 : tensor<1x512x7x7xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) outs(%72 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %74 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%73 : tensor<1x512x7x7xf32>) outs(%61 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %padded_118 = tensor.pad %74 low[0, 0, 1, 1] high[0, 0, 1, 1] {\n",
      "    ^bb0(%arg1: index, %arg2: index, %arg3: index, %arg4: index):\n",
      "      tensor.yield %cst_98 : f32\n",
      "    } : tensor<1x512x7x7xf32> to tensor<1x512x9x9xf32>\n",
      "    %75 = linalg.conv_2d_nchw_fchw {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%padded_118, %cst_5 : tensor<1x512x9x9xf32>, tensor<512x512x3x3xf32>) outs(%62 : tensor<1x512x7x7xf32>) -> tensor<1x512x7x7xf32>\n",
      "    cf.assert %3, \"training is not supported for now\"\n",
      "    %76 = linalg.generic {indexing_maps = [#map, #map1, #map1, #map1, #map1, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%75, %cst_2, %cst_1, %cst_4, %cst_3 : tensor<1x512x7x7xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>, tensor<512xf32>) outs(%75 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %in_120: f32, %in_121: f32, %in_122: f32, %out: f32):\n",
      "      %90 = arith.truncf %cst_100 : f64 to f32\n",
      "      %91 = arith.addf %in_122, %90 : f32\n",
      "      %92 = math.rsqrt %91 : f32\n",
      "      %93 = arith.subf %in, %in_121 : f32\n",
      "      %94 = arith.mulf %93, %92 : f32\n",
      "      %95 = arith.mulf %94, %in_119 : f32\n",
      "      %96 = arith.addf %95, %in_120 : f32\n",
      "      linalg.yield %96 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %77 = linalg.generic {indexing_maps = [#map2, #map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%76, %71 : tensor<1x512x7x7xf32>, tensor<1x512x7x7xf32>) outs(%61 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %78 = linalg.generic {indexing_maps = [#map2, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%77 : tensor<1x512x7x7xf32>) outs(%61 : tensor<1x512x7x7xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.cmpf ugt, %in, %cst_98 : f32\n",
      "      %91 = arith.select %90, %in, %cst_98 : f32\n",
      "      linalg.yield %91 : f32\n",
      "    } -> tensor<1x512x7x7xf32>\n",
      "    %79 = tensor.empty() : tensor<1x512x1x1xf32>\n",
      "    %80 = linalg.fill ins(%cst_98 : f32) outs(%79 : tensor<1x512x1x1xf32>) -> tensor<1x512x1x1xf32>\n",
      "    %81 = tensor.empty() : tensor<7x7xf32>\n",
      "    %82 = linalg.pooling_nchw_sum {dilations = dense<1> : vector<2xi64>, strides = dense<1> : vector<2xi64>} ins(%78, %81 : tensor<1x512x7x7xf32>, tensor<7x7xf32>) outs(%80 : tensor<1x512x1x1xf32>) -> tensor<1x512x1x1xf32>\n",
      "    %83 = linalg.generic {indexing_maps = [#map, #map], iterator_types = [\"parallel\", \"parallel\", \"parallel\", \"parallel\"]} ins(%82 : tensor<1x512x1x1xf32>) outs(%79 : tensor<1x512x1x1xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      %90 = arith.divf %in, %cst_101 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x512x1x1xf32>\n",
      "    %collapsed = tensor.collapse_shape %83 [[0], [1, 2, 3]] : tensor<1x512x1x1xf32> into tensor<1x512xf32>\n",
      "    %84 = tensor.empty() : tensor<512x1000xf32>\n",
      "    %85 = linalg.generic {indexing_maps = [#map3, #map4], iterator_types = [\"parallel\", \"parallel\"]} ins(%cst_0 : tensor<1000x512xf32>) outs(%84 : tensor<512x1000xf32>) {\n",
      "    ^bb0(%in: f32, %out: f32):\n",
      "      linalg.yield %in : f32\n",
      "    } -> tensor<512x1000xf32>\n",
      "    %86 = tensor.empty() : tensor<1x1000xf32>\n",
      "    %87 = linalg.fill ins(%cst_98 : f32) outs(%86 : tensor<1x1000xf32>) -> tensor<1x1000xf32>\n",
      "    %88 = linalg.matmul ins(%collapsed, %85 : tensor<1x512xf32>, tensor<512x1000xf32>) outs(%87 : tensor<1x1000xf32>) -> tensor<1x1000xf32>\n",
      "    %89 = linalg.generic {indexing_maps = [#map5, #map6, #map3], iterator_types = [\"parallel\", \"parallel\"]} ins(%88, %cst : tensor<1x1000xf32>, tensor<1000xf32>) outs(%86 : tensor<1x1000xf32>) {\n",
      "    ^bb0(%in: f32, %in_119: f32, %out: f32):\n",
      "      %90 = arith.addf %in, %in_119 : f32\n",
      "      linalg.yield %90 : f32\n",
      "    } -> tensor<1x1000xf32>\n",
      "    return %89 : tensor<1x1000xf32>\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compiled = torch_mlir.compile(symbolic_traced, torch.ones(1, 3, 224, 224), output_type=\"linalg-on-tensors\")\n",
    "print(compiled.operation.get_asm(large_elements_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mlir",
   "language": "python",
   "name": "torch-mlir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
